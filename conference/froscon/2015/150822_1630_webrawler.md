# Webcrawler

by florian & johannes from [systrix](http://www.sistrix.de/)

# agenda

* basics (how to build one)
* experience (what makes problem)
* future thoughts (what will happen)

# basics

## names

* crawler
* spider
* bot
* webcrawler
* searchbox
* ...

## notes

* 44 percentage of traffic is done by humans, rest by bots
* crawlers are only http clients
* netcat to wget to rss-reader to browser

## blueprint

* fetch the data
```php
curl_exec(curl_init($url));
```
* parse the data
    * io operation
    * xml and html parser
    * regular expression
    * editing (e.g. image optimisation)

## example

* fetch daily newest pictures from [qwertee](http://www.qwertee.com)
    * get html
    * use dom parser to fetch image urls by searching for right html property (e.g. css class "dynamic-image-design")
    * write a test to check if the expected content is returned (e.g. number of entries found by the css class)

# experience

* stay on the valid line / use the rules (e.g. respect robots.txt)
* don't put to much traffic and load on one system
* show who you are (user agent "<my company> crawler; http://www.foo.bar)
* treat the other server like you want that your servers are treated
* know your numbers
    * validate if your crawler use the known bandwith
    * if not, either memory or cpu is the bottleneck
* log everything
* speed up
    * do stuff in parallel
    * set timeout requests
    * add strategies to deal with hosts that are unreachable
    * use open connections
* choose the right environment
    * language
    * component
    * package
    * system

# future thoughts

* static html is gone
* dynamic html is comming up
    * CasperJS
    * PhantomJS
    * SlimerJS
    * HtmlUnit
    * PyQT4.QtWebKit
